{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting:\n",
    "``` Python\n",
    "from sklearn.model_selection import train_test_split \n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)  # Splits data into training/validation data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model types:\n",
    "```Python\n",
    "from sklearn.tree import DecisionTreeRegressor # Decision Tree\n",
    "from sklearn.ensemble import RandomForestRegressor  # Random forest model\n",
    "\n",
    "```\n",
    "#### Standard procedure:\n",
    "```Python\n",
    "model.fit(train_X,train_y)\n",
    "prediction = model.predict(val_X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Scoring:\n",
    "```Python\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(val_y, val_predictions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation:\n",
    "```Python\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()                 # can specify method here, \"median\", \"mean\", \"most_frequent\", “constant”,\n",
    "                                             # if \"constant\" then choose value with fill_value= num\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns    # These are the new sets you use for the model\n",
    "\n",
    "```\n",
    "An extension to imputing is include a column saying True/False for missing values, and use this with the imputed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "\n",
    "sns.set()  # Sets the graph theme to the seaborn style!\n",
    "sns.set_style(\"dark\") # 5 themes: (1)\"darkgrid\", (2)\"whitegrid\", (3)\"dark\", (4)\"white\", and (5)\"ticks\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/alexisbcook/choosing-plot-types-and-custom-styles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lineplot:\n",
    "``` Python\n",
    "sns.lineplot(data=dat_data['####'], label=\"####\")\n",
    "```\n",
    "\n",
    "### Barplot:\n",
    "``` Python\n",
    "sns.barplot(x=dat_data.index, y=dat_data['###'])                   # remember the index part for the x axis!!\n",
    "```\n",
    "\n",
    "### Heatmap: \n",
    "annot lets you show the value in the heatmap\n",
    "``` Python\n",
    "sns.heatmap(x=dat_data.index, annot=True)\n",
    "```\n",
    "\n",
    "### Scatterplot:\n",
    "Hue lets you colour the different data\n",
    "``` Python\n",
    "sns.scatterplot(x=dat_data['###'], y=dat_data['###'],hue=dat_data[\"ABC\"])\n",
    "```\n",
    "### Regressionplot:\n",
    "Just a scatter plot with a line of best fit\n",
    "``` Python\n",
    "sns.regplot(x=dat_data['###'], y=dat_data['###'])\n",
    "\n",
    "sns.lmplot(x=\"ABC\", y=\"DEF\", hue=\"HIJ\", data=dat_data,hue=)        # For separate lines of best fit\n",
    "\n",
    "sns.swarmplot(x=dat_data['ABC'],\n",
    "              y=dat_data['DEF'])              # Lets you have separate scatterplots on the same graph, shown vertically\n",
    "```\n",
    "\n",
    "### Histograms\n",
    "`a` tells us what column, and making kde true gives us a different plot\n",
    "``` Python\n",
    "sns.distplot(a=dat_data[\"ABC\"], kde=False, bins=)                  # can specify number of bins with bins=\n",
    "```\n",
    "\n",
    "### Density plots - KDE for Kernel Density Estimate\n",
    "`shade` shades in the area underneaththe density graph\n",
    "``` Python \n",
    "sns.kdeplot(data=dat_data['ABC'], shade=True)\n",
    "```\n",
    "### 2D KDE plots\n",
    "``` Python\n",
    "sns.jointplot(x=dat_data[\"ABC\"], y=dat_data[\"DEF\"], kind=\"kde\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2015 Sales    30\n",
       "2016 Sales    35\n",
       "2017 Sales    40\n",
       "Name: Product A, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([30, 35, 40], index=['2015 Sales', '2016 Sales', '2017 Sales'], name='Product A')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    ".shape                              # tells us the size of the data\n",
    ".dtype / dtypes                     # No brackets!!! just returns the datatype of column / datatypes in every entry\n",
    ".astype()                           # Lets you convert the datatype! examples to str, float64, int64, etc\n",
    ".describe()                         # gives a summary of the data\n",
    ".unique()                           # gives us all the unique values \n",
    ".value_counts()                     # shows us how often values occur sorted largest first\n",
    ".map()/apply()                      # Applys a function to all items in a column (map) / whole dataframe (apply)\n",
    ".idxmax()                           # returns index with the maximum value\n",
    ".replace(\"a\",\"b\")                   # lets you change entries from one to another\n",
    ".fillna()                           # Replaces all empty entries, method=bfill fills it with the next data in column\n",
    ".drop(,axis=)                       # Lets you drop rows/columns, choose with axis=\n",
    "pd.read_csv                         # opens the csv\n",
    "pandas.DataFrame.to_csv(obj,name)   # saves the csv\n",
    "\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "data[data.country.isin[\"ABC\",\"DEF\"]]              #  isin Lets you index categorically\n",
    "\n",
    "data[data.country.isnull[\"ABC\",\"DEF\"]]            #  isnull Lets you index empty entries (also notnull())\n",
    "\n",
    "cols_with_missing = [col for col in X_train.columns\n",
    "                     if X_train[col].isnull().any()]   # indexing columns containing empty entries\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grouping/sorting: https://www.kaggle.com/residentmario/grouping-and-sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can sometimes return multi-indexes, run .reset_index() to reset it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Python\n",
    ".groupby(CATEGORY)                      # Groups the entries that are identical in a column.\n",
    " -  example: reviews.groupby('points').price.min()  # gives you a minimum price for each point value\n",
    " -  reviews.groupby('points').size()                # similar to value_counts() but unsorted\n",
    "\n",
    ".agg([len, min, max])                               # lets you run a list of functions simultaneously\n",
    "\n",
    "\n",
    ".sort_values(by='len', ascending=False) # lets you sort values by a specific column, can sort by multiple columns!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### renaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Python\n",
    ".rename(columns/index={\"a\":\"b\"})              # changes the name of columns/index\n",
    ".rename_axis(\"a\",axis=\"rows\"/\"columns\")       # lets you change the name of the axis\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Python  \n",
    "pd.concat([a,b])                            # Lets you smush DataFrames together, useful when they have the same columns\n",
    "\n",
    ".join(data, lsuffix='_1', rsuffix='_2')     # Lets you combine DataFrames which have an index in common.\n",
    "                                            # The lsuffix and rsuffix lets you add names added on to separate columns\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling and Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Python\n",
    "# for min_max scaling (SCALING)\n",
    "from mlxtend.preprocessing import minmax_scaling\n",
    "\n",
    "# for Box-Cox Transformation (NORMALIZATION)\n",
    "from scipy import stats\n",
    "```        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **scaling** -  changing the range of your data\n",
    " - **normalization** - changing the shape of the distribution of your data to a bell curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using methods that base on measuring how far away data points are:\n",
    " -  support vector machines (SVN)\n",
    " -  k-nearest neighbors (KNN)\n",
    " ```Python\n",
    "scaled_data = minmax_scaling(original_data, columns=[\"ABC\"])      # scales between 0-1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using methods that assume your data is normally distributed:\n",
    "-  linear discriminant analysis (LDA)\n",
    "-  Gaussian naive Bayes\n",
    "<br>\n",
    "Anything that says Gaussian most likely assumes normally distributed data\n",
    "\n",
    "```Python\n",
    "normalized_data = stats.boxcox(original_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# how many total missing values do we have?\n",
    "total_cells = np.product(dat_data.shape)          #number of all cells\n",
    "total_missing = dat_data.isnull().sum()\n",
    "# percent of data that is missing\n",
    "percent_missing = (total_missing/total_cells) * 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All null functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    ".fillna()                           # Replaces all empty entries\n",
    "                                    method= \"bfill\", axis = 0  # fills it with the next data in column\n",
    ".dropna()                           # Removes any row/column with empty entries, specify with axis=\n",
    ".isnull()                           # Lets you index empty entries  .isna()\n",
    ".notnull()                          # Lets you index non empty entries\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note this onlyworks if all the dates are in the same format\n",
    "```Python\n",
    "data[\"Dates\"] = pd.to_datetime(data[\"Dates\"], format=\"%m/%d/%y\") \n",
    "```\n",
    "3/2/07 $\\rightarrow$ 2007-03-02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract information:\n",
    "```Python\n",
    "data[\"Dates\"].dt.day                 # returns a list of the dates\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if dates are all the same format:\n",
    "```Python\n",
    "date_lengths = data.Dates.str.len()\n",
    "date_lengths.value_counts()\n",
    "\n",
    "returns something like :\n",
    "10    23409\n",
    "24        3\n",
    "Name: Date, dtype: int64\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could be useful:\n",
    "```Python\n",
    "pandas.read_csv(parse_dates=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character encoding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utf-8 is the standard str encoding for saving in pandas!\n",
    "```Python\n",
    "import chardet                            # character encode module\n",
    "text.decode(\"big5-tw\")                    # decodes a string for python to print\n",
    "text.encode(\"utf-8\", errors=\"replace\")    # this lets you encode to utf-8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an error occurs when opening a file as utf-8, then the file is encoded in something else:\n",
    "```Python\n",
    "\n",
    "with open(\"data.csv\", 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(10000))           # increase this value if it returns the wrong encoding\n",
    "    \n",
    "    # searches for all possible encodings\n",
    "pd.read_csv(\"data.csv\", encoding=\"\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inconsistent Data Entry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, South Korea is written as southkorea, south korea and South Korea. we can clean this up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "import fuzzywuzzy                  # used to identify which strings are closest to each other\n",
    "from fuzzywuzzy import process\n",
    "import chardet                     # for character encoding\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    ".str.strip()                           # removes spaces at the start and end of the string\n",
    ".str.lower/upper()                     # converts string to lower/upper case\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "matches = fuzzywuzzy.process.extract(\"south korea\", countries, limit=5, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This complicated function returns something like: Just a score for the strings closest to \"south korea\"\n",
    "```Python\n",
    "[('south korea', 100),\n",
    " ('southkorea', 48),\n",
    " ('saudi arabia', 43),\n",
    " ('norway', 35),\n",
    " ('austria', 33),\n",
    " ('ireland', 33),\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing the strings knowing the threshold is 48:\n",
    "```Python\n",
    "def replace_matches_in_column(df, column, string_to_match, min_ratio = 47):\n",
    "    # get a list of unique strings\n",
    "    strings = df[column].unique()\n",
    "    \n",
    "    # get the top 10 closest matches to our input string\n",
    "    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n",
    "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "    # only get matches with a ratio > 90\n",
    "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n",
    "\n",
    "    # get the rows of all the close matches in our dataframe\n",
    "    rows_with_matches = df[column].isin(close_matches)\n",
    "\n",
    "    # replace all rows with close matches with the input matches \n",
    "    df.loc[rows_with_matches, column] = string_to_match\n",
    "    \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
